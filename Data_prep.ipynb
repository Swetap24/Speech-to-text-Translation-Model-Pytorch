{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_train = np.load('train file path', allow_pickle=True, encoding='bytes')\n",
    "speech_valid = np.load('validation file path', allow_pickle=True, encoding='bytes')\n",
    "speech_test = np.load('test file path', allow_pickle=True, encoding='bytes')\n",
    "\n",
    "transcript_train = np.load('train transcripts file for testing', allow_pickle=True,encoding='bytes')\n",
    "transcript_valid = np.load('validation transcripts file for testing', allow_pickle=True,encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_list = [' ','A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q',\\\n",
    "             'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '.', '_', '+', ' ','<sos>','<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=transcript_train\n",
    "alls=[]\n",
    "for i, each in enumerate(a):\n",
    "    m=[]\n",
    "    for p, item in enumerate(a[i]):\n",
    "        for f, item2 in enumerate(a[i][p]):\n",
    "          if p==0 and f==0:\n",
    "              m.insert(0, '<sos>')\n",
    "              m.append(a[i][p].decode()[f])\n",
    "          elif p==len(a[i])-1 and f==len(a[i][p])-1:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "              m.append('<eos>')\n",
    "          elif f==len(a[i][p])-1:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "              m.append(\" \")\n",
    "          else:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "    alls.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=transcript_valid\n",
    "allsv=[]\n",
    "for i, each in enumerate(a):\n",
    "    m=[]\n",
    "    for p, item in enumerate(a[i]):\n",
    "        for f, item2 in enumerate(a[i][p]):\n",
    "          if p==0 and f==0:\n",
    "              m.insert(0, '<sos>')\n",
    "              m.append(a[i][p].decode()[f])\n",
    "          elif p==len(a[i])-1 and f==len(a[i][p])-1:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "              m.append('<eos>')\n",
    "          elif f==len(a[i][p])-1:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "              m.append(\" \")\n",
    "          else:\n",
    "              m.append(a[i][p].decode()[f])\n",
    "    allsv.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_letter_to_index(transcript, letter_list):\n",
    "    '''\n",
    "    :param transcript :(N, ) Transcripts are the text input\n",
    "    :param letter_list: Letter list defined above\n",
    "    :return letter_to_index_list: Returns a list for all the transcript sentence to index\n",
    "    '''\n",
    "    letter_to_index_list=[]   \n",
    "    for i, o in enumerate(transcript):\n",
    "      index_list=[] \n",
    "      for s, a in enumerate(transcript[i]):\n",
    "          check =  all(a in letter_list for item in transcript[i])\n",
    "          if check is True:\n",
    "              index_list.append(letter_list.index(a))\n",
    "      letter_to_index_list.append(index_list)\n",
    "    return letter_to_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_text_train = transform_letter_to_index(alls, letter_list)\n",
    "character_text_valid = transform_letter_to_index(allsv, letter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech2Text_Dataset(Dataset):\n",
    "  def __init__(self, speech, text=None, train=True):\n",
    "    self.speech = speech\n",
    "    self.train = train\n",
    "    if(text is not None):\n",
    "      self.text = text\n",
    "  def __len__(self):\n",
    "    return self.speech.shape[0]\n",
    "  def __getitem__(self, index):\n",
    "    if(self.train):\n",
    "      text = self.text[index]\n",
    "      return torch.tensor(self.speech[index].astype(np.float32)), torch.tensor(text[:-1]), torch.tensor(text[1:])\n",
    "    else:\n",
    "      return torch.tensor(self.speech[index].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_train(batch_data):\n",
    "    inputs, text_input, Labels = zip(*batch_data)\n",
    "    #Input\n",
    "    lens=[len(seq) for seq in inputs]\n",
    "    inputs=[inputs[i] for i in range(len(lens))]\n",
    "    Len1=torch.LongTensor([len(inp) for inp in inputs])\n",
    "    inputs=utils.rnn.pad_sequence(inputs)\n",
    "    #Pred\n",
    "    lens2=[len(seq) for seq in text_input]\n",
    "    text_input=[text_input[i] for i in range(len(lens2))]\n",
    "    Len2=torch.LongTensor([len(inp) for inp in text_input])\n",
    "    text_input = utils.rnn.pad_sequence(text_input)\n",
    "    #True\n",
    "    lens3=[len(seq) for seq in Labels]   \n",
    "    Labels=[Labels[i] for i in range(len(lens3))]   \n",
    "    Len3=torch.LongTensor([len(inp) for inp in Labels])\n",
    "    Labels = utils.rnn.pad_sequence(Labels)\n",
    "    \n",
    "    return inputs, text_input, Labels, Len1, Len2, Len3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test(batch_data):\n",
    "    x=batch_data\n",
    "    lens=[len(seq) for seq in x]\n",
    "    x1=sorted(range(len(lens)), key=lens.__getitem__, reverse=True)\n",
    "    x=[x[i] for i in x1]\n",
    "    len1=torch.LongTensor([len(inp) for inp in x])\n",
    "    x=utils.rnn.pad_sequence(x)\n",
    "    return x, len1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Speech2Text_train_Dataset = Speech2Text_Dataset(speech_train, character_text_train)\n",
    "Speech2Text_val_Dataset = Speech2Text_Dataset(speech_valid, character_text_valid)\n",
    "Speech2Text_test_Dataset = Speech2Text_Dataset(speech_test, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Speech2Text_train_Dataset, batch_size=64, shuffle=True, collate_fn=collate_train)\n",
    "val_loader = DataLoader(Speech2Text_val_Dataset, batch_size=1, shuffle=False, collate_fn=collate_train)\n",
    "test_loader = DataLoader(Speech2Text_test_Dataset, batch_size=1, shuffle=False, collate_fn=collate_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
